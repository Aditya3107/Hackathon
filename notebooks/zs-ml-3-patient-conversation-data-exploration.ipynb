{"cells":[{"metadata":{"trusted":true},"cell_type":"code","source":"! pip install tldextract","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import defaultdict,Counter\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom tldextract import extract\nimport re","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#nltk libraries\nimport nltk\nfrom nltk.stem import WordNetLemmatizer,PorterStemmer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nnltk.download('stopwords')\nstop=set(stopwords.words('english'))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/zsdataset/train.csv', encoding = 'ISO-8859-1')\ntest_df = pd.read_csv('../input/zsdataset/test.csv', encoding = 'ISO-8859-1')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Take a look into number of columns and rows of dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"print('The shape of training dataset is = {}'.format(train_df.shape))\nprint('The shape of testing dataset is = {}'.format(test_df.shape))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Exploring the training and testing datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"#info of training dataset\ntrain_df.info()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#info of testing dataset\ntest_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Drop the duplicate values in training dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.drop_duplicates()\ntrain_df.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Check one last column of testing dataframe 'Unnamed: 9', because as per above test dataset info, it has only 1 non null count, and it won't be adding any information, so there is no harm in dropping it. Also check out the index in test_df, it just showing the index number in test dataset, as of now we will remove it. "},{"metadata":{"trusted":true},"cell_type":"code","source":"print(test_df[['Unnamed: 9']].head())\ntest_df.drop(columns = ['Index','Unnamed: 9'],inplace = True)\ntest_df.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.info()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now looking at NULL and NAN values, and before that we will add up train and test dataset and we will put a column named 'Patient_Tag' in test dataset and fill it with -1. "},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df_1 = test_df.copy()\ntest_df_1['Patient_Tag'] = -1\ndf = pd.concat([train_df, test_df_1])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Null values\nnull = df.isnull().sum().sort_values(ascending =True)\n#percentage missing\npercentage_missing = ((df.isnull().sum()/df.shape[0])*100).sort_values(ascending= False)\npercentage_missing","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"missing_data = pd.concat([null,percentage_missing],axis = 1,keys = ['Total missing', 'Percent missing'])\nmissing_data","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Lets check heatmap of NULL values. Also note that, one value of TRANS_CONV_TEXT is missing. "},{"metadata":{"trusted":true},"cell_type":"code","source":"sns.heatmap(df.isnull(), yticklabels=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Checkout the Patient Tag"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.Patient_Tag.value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class_df = train_df.groupby('Patient_Tag').count()['TRANS_CONV_TEXT'].reset_index().sort_values(by = 'TRANS_CONV_TEXT', ascending = False)\nclass_df.style.background_gradient(cmap = 'winter')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"percent_class=class_df.TRANS_CONV_TEXT\nlabels= class_df.Patient_Tag\n\ncolors = ['#25C38B','#F92725']\n\npie,_,_ = plt.pie(percent_class,radius = 1.0,labels=labels,colors=colors,autopct=\"%.1f%%\")\nplt.setp(pie, width=0.6, edgecolor='grey') \nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's explore the text data"},{"metadata":{},"cell_type":"markdown","source":"First we will clean all the text data"},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(x):\n    normalizedsentense = x.lower()\n    text = re.sub(r\"[^a-z']+\", ' ', normalizedsentense)\n    return text\ntrain_df['TRANS_CONV_TEXT']=train_df['TRANS_CONV_TEXT'].astype(str)\ntest_df['TRANS_CONV_TEXT']=test_df['TRANS_CONV_TEXT'].astype(str)\ntrain_df['TRANS_CONV_TEXT']=train_df['TRANS_CONV_TEXT'].apply(lambda x:clean_text(x))\ntest_df['TRANS_CONV_TEXT']=test_df['TRANS_CONV_TEXT'].apply(lambda x:clean_text(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df['TRANS_CONV_TEXT']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\n\ntext_len=train_df[train_df['Patient_Tag']== 0]['TRANS_CONV_TEXT'].str.len()\nax1.hist(text_len,color='#25C38B')\nax1.set_title('Patient Tag 0')\n\ntext_len=train_df[train_df['Patient_Tag']== 1]['TRANS_CONV_TEXT'].str.len()\nax2.hist(text_len,color='#F92725')\nax2.set_title('Patient Tag 1')\n\nfig.suptitle('Characters in text')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It can be seen from the graph that, the patient tag 0 does have, max length of text is upto 16000, whereas the patient tag 1 has max lenth of text is up to 12000"},{"metadata":{},"cell_type":"markdown","source":"Let's check number of words in Text"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df_1 = train_df.copy()\ntrain_df_1['text'] = train_df_1.TRANS_CONV_TEXT\ntrain_df_1[\"text\"] = train_df_1[\"text\"].astype(str)\n\nfig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\n\ntext_len=train_df_1[train_df_1['Patient_Tag']==0]['text'].str.split().map(lambda x: len(x))\nax1.hist(text_len,color='#17C37B')\nax1.set_title('Patient Tag 0')\n\ntext_len=train_df_1[train_df_1['Patient_Tag']==1]['text'].str.split().map(lambda x: len(x))\nax2.hist(text_len,color='#F92969')\nax2.set_title('Patient Tag 1')\n\nfig.suptitle('Words in Text')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check some most common words in text"},{"metadata":{"trusted":true},"cell_type":"code","source":"def create_corpus(target):\n    corpus=[]\n    \n    for x in train_df_1[train_df_1['Patient_Tag']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#check stop words, and we will remove them while counting all other words\nnp.array(stop)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus=create_corpus(0)\ncounter=Counter(corpus)\nmost=counter.most_common()\nx=[]\ny=[]\nfor word,count in most[:50]:\n    if (word.lower() not in stop) :\n        x.append(word)\n        y.append(count)\nsns.barplot(x=y,y=x)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"corpus=create_corpus(1)\ncounter=Counter(corpus)\nmost=counter.most_common()\nx=[]\ny=[]\nfor word,count in most[:50]:\n    if (word.lower() not in stop) :\n        x.append(word)\n        y.append(count)\nsns.barplot(x=y,y=x)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"It seems it's all about heart, blood, failure and risks. : -p"},{"metadata":{},"cell_type":"markdown","source":"Let's make some word cloud"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df_2 = train_df.copy()\ntrain_df_2['text'] = train_df_2.TRANS_CONV_TEXT\ntrain_df_2[\"text\"] = train_df_2[\"text\"].astype(str)\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=[20, 10])\n\ntrain_df_2_0 = train_df_2[train_df_2[\"Patient_Tag\"]==0]\ntrain_df_2_1 = train_df_2[train_df_2[\"Patient_Tag\"]==1]\n\n\ncomment_words = '' \nstopwords = set(STOPWORDS) \n\nfor val in train_df_2_0.text: \n      \n    # typecaste each val to string \n    val = str(val) \n  \n    # split the value \n    tokens = val.split() \n      \n    # Converts each token into lowercase \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n      \n    comment_words += \" \".join(tokens)+\" \"\n\nwordcloud1 = WordCloud(width = 800, height = 800, \n                background_color ='white',\n                colormap=\"Greens\",\n                stopwords = stopwords, \n                min_font_size = 10).generate(comment_words) \n\nax1.imshow(wordcloud1)\nax1.axis('off')\nax1.set_title('Patient_Tag 0',fontsize=35);\n\ncomment_words = ''\n\nfor val in train_df_2_1.text: \n      \n    # typecaste each val to string \n    val = str(val) \n  \n    # split the value \n    tokens = val.split() \n      \n    # Converts each token into lowercase \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n      \n    comment_words += \" \".join(tokens)+\" \"\n\n\n\nwordcloud2 = WordCloud(width = 800, height = 800, \n                background_color ='white',\n                colormap=\"Reds\",\n                stopwords = stopwords, \n                min_font_size = 10).generate(comment_words)  \nax2.imshow(wordcloud2)\nax2.axis('off')\nax2.set_title('Patient_Tag 1',fontsize=35);\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now let's explore, sources"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df_3 = train_df.copy()\ntrain_df_3['Source'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['Source'].value_counts()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Make Facebook and FECEBOOK both are same in both train and test dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"def classes_def(x):\n    if x ==  \"FACEBOOK\":\n        return \"Facebook\"\n    elif x == 'FORUMS':\n        return 'Forums'\n    elif x == 'BLOG':\n        return 'Blog'\n    elif x == 'YOUTUBE':\n        return 'Youtube'\n    else:\n        return 'Facebook'\ntrain_df['Source']=train_df['Source'].apply(lambda x:classes_def(x))\ntest_df['Source']=test_df['Source'].apply(lambda x:classes_def(x))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"source_data = train_df['Source'].value_counts()\nsource_data","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,1, figsize=(7, 6))\nsource_data = train_df['Source'].value_counts()\nax.bar(source_data.index, source_data, width = 0.35, edgecolor = 'white',linewidth=0.3,color = '#A1D539')\nfor i in source_data.index:\n    ax.annotate(f\"{source_data[i]}\", \n                   xy=(i, source_data[i] +10),\n                   va = 'center', ha='center',fontweight='heavy', fontfamily='roman',\n                   color='#F92725')\n    \nfor s in ['top','right']:\n    ax.spines[s].set_visible(False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now look at the hosts"},{"metadata":{},"cell_type":"markdown","source":"As we saw earlier, there are null values in host but there are no null values in link. So we will copy the link directly and extract the host name where, there is null value in host, we will do this in both train and test datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df[\"Host\"].fillna(train_df[\"Link\"], inplace=True)\ntest_df[\"Host\"].fillna(test_df[\"Link\"], inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#extracts the url from the link, we will extract hostname only\ndef extract_url(x):\n    tsd, td, tsu = extract(x) # prints abc, hostname, com\n    return td","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['Host']=train_df['Host'].apply(lambda x:extract_url(x))\ntrain_df['Host'].value_counts().sort_values(ascending= False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"We see there is one entry with 65 values is an empty string, if we check in the data, they are from the host Youtube. So we will manually put the host name youtube."},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['Host']=test_df['Host'].apply(lambda x:extract_url(x))\ntest_df['Host'].value_counts().sort_values(ascending= False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_df['Host'] = train_df['Host'].apply(lambda 'unknown' : train_df[train_df['Host'] == '']\ntrain_df.loc[train_df.Host == '' , 'Host'] = 'youtube'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"host = train_df['Host'].value_counts().nlargest(n=10)\nfig, ax = plt.subplots(1,1, figsize=(15, 4))\n\nax.bar(host.index, host, width = 0.5, edgecolor = 'white',linewidth=0.3)\nfor i in host.index:\n    ax.annotate(f\"{host[i]}\", \n                   xy=(i, host[i] + 5),\n                   va = 'center', ha='center',fontweight='heavy', fontfamily='serif',\n                   color='#0B0B0B')\n    \nfor s in ['top','left','right','bottom']:\n    ax.spines[s].set_visible(False)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"host_test = test_df['Host'].value_counts().nlargest(n=10)\nfig, ax = plt.subplots(1,1, figsize=(15, 5))\n\nax.bar(host_test.index, host, width = 0.5, edgecolor = 'white',linewidth=0.3, color = '#EAA1F4')\nfor i in host_test.index:\n    ax.annotate(f\"{host_test[i]}\", \n                   xy=(i, host_test[i] + 10),\n                   va = 'center', ha='center',fontweight='heavy', fontfamily='serif',\n                   color='#0B0B0B')\n    \nfor s in ['top','left','right','bottom']:\n    ax.spines[s].set_visible(False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Also, there is no meaning to keep links in data, as we already keep host name in the dataset. Keeping URL  will not add any difference. "},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.drop(['Link'], axis=1)\ntest_df = test_df.drop(['Link'],axis = 1)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now look at date and time"},{"metadata":{},"cell_type":"markdown","source":"The Eastern Time Zone (ET) is an area 5 hours behind Greenwich Mean Time (GMT-5) during the winter months (referred to as Eastern Standard Time or EST) and 4 hours behind Greenwich Mean Time (GMT-4) during the summer months (referred to as Eastern Daylight Time or EDT)."},{"metadata":{"trusted":true},"cell_type":"code","source":"date_time = train_df[['Date(ET)','Time(ET)','time(GMT)']]\ndate_time","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As it can be seen that, ET is 5 hours behind than GMT. Both columns Time(ET) and time(GMT) finally gives a time, which is same. There is no meaning to keep both columns, so we will remove one time(GMT) column. Another reason to remove time(GMT) is, it has 252 missing values."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.drop(['time(GMT)'], axis=1)\ntest_df = test_df.drop(['time(GMT)'],axis = 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#train_df['Date(ET)'] =  pd.to_datetime(train_df['Date(ET)'],format='%dd-%mm-%yyyy')\ntrain_df['Date(ET)'] = pd.to_datetime(train_df['Date(ET)'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['Time(ET)'] = pd.to_datetime(train_df['Time(ET)'],errors='coerce').dt.time\ntest_df['Time(ET)'] = pd.to_datetime(test_df['Time(ET)'],errors='coerce').dt.time","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Due to COERCE, where there is time like 0.87 or 0.47 it will be converted to NAT and we will put the median time on their place. "},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df['Time(ET)'].isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['Time(ET)'].isna().sum()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_train_df = train_df.dropna()\ntemp_test_df = test_df.dropna()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At the end we can add one more feature according to time and date. "},{"metadata":{"trusted":true},"cell_type":"code","source":"temp_train_df.loc[:,'hour'] = pd.to_datetime(temp_train_df['Time(ET)'], format='%H:%M:%S')\ntemp_train_df.loc[:,'hour'] = temp_train_df['hour'].dt.hour","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"prods = pd.DataFrame({'hour':range(1, 25)})\nb = [0,4,8,12,16,20,24]\nl = ['Late Night', 'Early Morning','Morning','Noon','Eve','Night']\nprods['session'] = pd.cut(prods['hour'], bins=b, labels=l, include_lowest=True)\ndef f(x):\n    if (x > 6) and (x <= 12):\n        return 'Morning'\n    elif (x > 12) and (x <= 18 ):\n        return 'Noon'\n    elif (x > 18) and (x <= 24):\n        return'Evening'\n    elif (x > 0) and (x <= 6) :\n        return 'Night'\ntemp_train_df['hour'] = temp_train_df['hour'].apply(f)\ntemp_train_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"fig, ax = plt.subplots(1,1, figsize=(7, 6))\nhourly_data = temp_train_df['hour'].value_counts()\nax.bar(hourly_data.index, hourly_data, width = 0.35, edgecolor = 'white',linewidth=0.3,color = '#C1B539')\nfor i in hourly_data.index:\n    ax.annotate(f\"{hourly_data[i]}\", \n                   xy=(i, hourly_data[i] +10),\n                   va = 'center', ha='center',fontweight='heavy', fontfamily='roman',\n                   color='#F58624')\n    \nfor s in ['top','right']:\n    ax.spines[s].set_visible(False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model Generation"},{"metadata":{},"cell_type":"markdown","source":"First we will make a model with Text data only, then afterwords we can add more features in them"},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}