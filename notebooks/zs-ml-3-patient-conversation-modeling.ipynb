{"cells":[{"metadata":{},"cell_type":"markdown","source":"We will start with cleaning the datasets. As we did in data exploration note book. "},{"metadata":{"trusted":true},"cell_type":"code","source":"! pip install tldextract","execution_count":1,"outputs":[{"output_type":"stream","text":"Collecting tldextract\n  Downloading tldextract-3.1.0-py2.py3-none-any.whl (87 kB)\n\u001b[K     |████████████████████████████████| 87 kB 956 kB/s eta 0:00:01\n\u001b[?25hRequirement already satisfied: requests>=2.1.0 in /opt/conda/lib/python3.7/site-packages (from tldextract) (2.25.1)\nCollecting requests-file>=1.4\n  Downloading requests_file-1.5.1-py2.py3-none-any.whl (3.7 kB)\nRequirement already satisfied: filelock>=3.0.8 in /opt/conda/lib/python3.7/site-packages (from tldextract) (3.0.12)\nRequirement already satisfied: idna in /opt/conda/lib/python3.7/site-packages (from tldextract) (2.10)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.1.0->tldextract) (2020.12.5)\nRequirement already satisfied: chardet<5,>=3.0.2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.1.0->tldextract) (3.0.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.1.0->tldextract) (1.26.2)\nRequirement already satisfied: six in /opt/conda/lib/python3.7/site-packages (from requests-file>=1.4->tldextract) (1.15.0)\nInstalling collected packages: requests-file, tldextract\nSuccessfully installed requests-file-1.5.1 tldextract-3.1.0\n\u001b[33mWARNING: You are using pip version 21.0; however, version 21.0.1 is available.\nYou should consider upgrading via the '/opt/conda/bin/python3.7 -m pip install --upgrade pip' command.\u001b[0m\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom tldextract import extract\nimport re,sys,pickle","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#nltk libraries\nimport nltk\nfrom nltk.stem import WordNetLemmatizer,PorterStemmer\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nnltk.download('stopwords')\nstop=set(stopwords.words('english'))","execution_count":3,"outputs":[{"output_type":"stream","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = pd.read_csv('../input/zsdataset/train.csv',encoding= 'ISO-8859-1')\ntest_df = pd.read_csv('../input/zsdataset/test.csv',encoding= 'ISO-8859-1')","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"As we did in data exploration note book we will clean our dataset. "},{"metadata":{"trusted":true},"cell_type":"code","source":"#extracts the url from the link, we will extract hostname only\ndef extract_url(x):\n    tsd, td, tsu = extract(x) # prints abc, hostname, com\n    return td\ndef classes_def(x):\n    if x ==  \"FACEBOOK\":\n        return \"Facebook\"\n    elif x == 'FORUMS':\n        return 'Forums'\n    elif x == 'BLOG':\n        return 'Blog'\n    elif x == 'YOUTUBE':\n        return 'Youtube'\n    else:\n        return 'Facebook'","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['Source']=train_df['Source'].apply(lambda x:classes_def(x))\ntest_df['Source']=test_df['Source'].apply(lambda x:classes_def(x))\ntrain_df[\"Host\"].fillna(train_df[\"Link\"], inplace=True)\ntest_df[\"Host\"].fillna(test_df[\"Link\"], inplace=True)\ntrain_df['Host']=train_df['Host'].apply(lambda x:extract_url(x))\ntest_df['Host']=test_df['Host'].apply(lambda x:extract_url(x))\ntrain_df.loc[train_df.Host == '' , 'Host'] = 'youtube'","execution_count":7,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's drop columns which are not useful, we will be dropping Link, time(GMT), as it gives same information as Host and Time(ET) respectively. Also, Title has maximum 20% Null values. It can be affect the training of dataset. So we will drop it too"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df.drop(['Link','time(GMT)','Title'],axis= 1)\ntest_df = test_df.drop(['Link','time(GMT)','Title'],axis= 1)","execution_count":8,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At last, make date and time columns, in date-time format."},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['Date(ET)'] = pd.to_datetime(train_df['Date(ET)'],errors='coerce').dt.date\ntest_df['Date(ET)'] = pd.to_datetime(test_df['Date(ET)'],errors='coerce').dt.date","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['Time(ET)'] = pd.to_datetime(train_df['Time(ET)'],errors='coerce').dt.time\ntest_df['Time(ET)'] = pd.to_datetime(test_df['Time(ET)'],errors='coerce').dt.time","execution_count":10,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Now we will have, NaT elements in date and time. We won't remove them now as we are not adding this features while generating model. But, later if we use these features then, we will remove NaT rows from dataset. "},{"metadata":{},"cell_type":"markdown","source":"Also remove Unnamed:9 and Index columns from test datasets"},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.drop(columns = ['Index','Unnamed: 9'],inplace = True)","execution_count":11,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Also, there is one row in TRANS_CONV_TEXT is having null, we will drop that row"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df = train_df[train_df['TRANS_CONV_TEXT'].notna()]\ntrain_df.info()","execution_count":12,"outputs":[{"output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\nInt64Index: 1156 entries, 0 to 1156\nData columns (total 6 columns):\n #   Column           Non-Null Count  Dtype \n---  ------           --------------  ----- \n 0   Source           1156 non-null   object\n 1   Host             1156 non-null   object\n 2   Date(ET)         1156 non-null   object\n 3   Time(ET)         1084 non-null   object\n 4   TRANS_CONV_TEXT  1156 non-null   object\n 5   Patient_Tag      1156 non-null   int64 \ndtypes: int64(1), object(5)\nmemory usage: 63.2+ KB\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.info()","execution_count":13,"outputs":[{"output_type":"stream","text":"<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 571 entries, 0 to 570\nData columns (total 5 columns):\n #   Column           Non-Null Count  Dtype \n---  ------           --------------  ----- \n 0   Source           571 non-null    object\n 1   Host             571 non-null    object\n 2   Date(ET)         570 non-null    object\n 3   Time(ET)         532 non-null    object\n 4   TRANS_CONV_TEXT  571 non-null    object\ndtypes: object(5)\nmemory usage: 22.4+ KB\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"The last thing is data preprocessing. We will clean text and remove, all numbers and punctuations from the text data, TRANS_CONV_TEXT"},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_text(x):\n    normalizedsentense = x.lower()\n    text = re.sub(r\"[^a-z']+\", ' ', normalizedsentense)\n    return text","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df['TRANS_CONV_TEXT']=train_df['TRANS_CONV_TEXT'].apply(lambda x:clean_text(x))\ntest_df['TRANS_CONV_TEXT']=test_df['TRANS_CONV_TEXT'].apply(lambda x:clean_text(x))","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_df.head()","execution_count":16,"outputs":[{"output_type":"execute_result","execution_count":16,"data":{"text/plain":"   Source                Host    Date(ET)  Time(ET)  \\\n0  Forums          cafepharma  2016-06-15  13:58:00   \n1  Forums             patient  2016-05-07       NaT   \n2    Blog  abcnewsradioonline  2016-04-14  15:00:38   \n3  Forums       cancer-forums  2016-06-18  20:46:00   \n4  Forums            diyaudio  2016-06-15  03:26:00   \n\n                                     TRANS_CONV_TEXT  Patient_Tag  \n0  i don't disagree with you in principle i'm jus...            0  \n1  i am always dizzy i get dizzy standing up so i...            1  \n2  axelle bauer griffin filmmagic new york queen ...            0  \n3  i am and i have been throwing up for about a y...            1  \n4  quote originally posted by boyan silyavski wak...            0  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Source</th>\n      <th>Host</th>\n      <th>Date(ET)</th>\n      <th>Time(ET)</th>\n      <th>TRANS_CONV_TEXT</th>\n      <th>Patient_Tag</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Forums</td>\n      <td>cafepharma</td>\n      <td>2016-06-15</td>\n      <td>13:58:00</td>\n      <td>i don't disagree with you in principle i'm jus...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Forums</td>\n      <td>patient</td>\n      <td>2016-05-07</td>\n      <td>NaT</td>\n      <td>i am always dizzy i get dizzy standing up so i...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Blog</td>\n      <td>abcnewsradioonline</td>\n      <td>2016-04-14</td>\n      <td>15:00:38</td>\n      <td>axelle bauer griffin filmmagic new york queen ...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Forums</td>\n      <td>cancer-forums</td>\n      <td>2016-06-18</td>\n      <td>20:46:00</td>\n      <td>i am and i have been throwing up for about a y...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Forums</td>\n      <td>diyaudio</td>\n      <td>2016-06-15</td>\n      <td>03:26:00</td>\n      <td>quote originally posted by boyan silyavski wak...</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_df.head()","execution_count":17,"outputs":[{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"   Source          Host    Date(ET)  Time(ET)  \\\n0    Blog   uhmagonline  2016-07-30  00:41:23   \n1  Forums          yuku  2016-06-20  00:07:30   \n2    Blog      blogspot  2016-06-15  15:44:00   \n3  Forums  healthboards  2016-07-17  19:41:00   \n4    Blog  sciencecodex  2016-04-04  15:30:45   \n\n                                     TRANS_CONV_TEXT  \n0  baby slice the son of the late kimbo slice has...  \n1   p font face sans serif size i have had both s...  \n2  previously sodium glucose cotransporter sglt i...  \n3  hello i suffer from congestive heart failure d...  \n4  a daily dose of vitamin d improves heart funct...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Source</th>\n      <th>Host</th>\n      <th>Date(ET)</th>\n      <th>Time(ET)</th>\n      <th>TRANS_CONV_TEXT</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Blog</td>\n      <td>uhmagonline</td>\n      <td>2016-07-30</td>\n      <td>00:41:23</td>\n      <td>baby slice the son of the late kimbo slice has...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Forums</td>\n      <td>yuku</td>\n      <td>2016-06-20</td>\n      <td>00:07:30</td>\n      <td>p font face sans serif size i have had both s...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>Blog</td>\n      <td>blogspot</td>\n      <td>2016-06-15</td>\n      <td>15:44:00</td>\n      <td>previously sodium glucose cotransporter sglt i...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>Forums</td>\n      <td>healthboards</td>\n      <td>2016-07-17</td>\n      <td>19:41:00</td>\n      <td>hello i suffer from congestive heart failure d...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Blog</td>\n      <td>sciencecodex</td>\n      <td>2016-04-04</td>\n      <td>15:30:45</td>\n      <td>a daily dose of vitamin d improves heart funct...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Now it looks good. We will start modeling."},{"metadata":{},"cell_type":"markdown","source":"### Let's create a model with Logistic regression and bag of words and 5 kfolds"},{"metadata":{},"cell_type":"markdown","source":"* Bag of Words (BOW) is a method to extract features from text documents. These features can be used for training machine learning algorithms. It creates a vocabulary of all the unique words occurring in all the documents in the training set. \n*  logistic regression is one of the most important analytic tools in the social and natural sciences. In natural language processing, logistic regression is the baseline supervised machine learning algorithm for classification, and also has a very close relationship with neural networks."},{"metadata":{"trusted":true},"cell_type":"code","source":"df = train_df.copy()\ndf_test = test_df.copy()","execution_count":62,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nfrom sklearn import linear_model\nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\n\n\nif __name__ ==\"__main__\":\n    #create a new column with and fill it with -1\n    df['kfold'] = -1\n    #randomize the data\n    df = df.sample(frac = 1).reset_index(drop =True)\n    y = df.Patient_Tag.values\n    #initiate model selection class from stratified kfold module\n    kf = model_selection.StratifiedKFold(n_splits = 5)\n    \n    # fill the new k fold column\n    for f,(t_,v_) in enumerate(kf.split(X =df,y = y)):\n        df.loc[v_,'kfold'] = f\n\n    #go over the folds created\n    for fold_ in range(5):\n        \n        #creating temporary dataframes for train and test\n        train_data = df[df.kfold != fold_].reset_index(drop = True)\n        test_data = df[df.kfold == fold_].reset_index(drop = True)\n\n        #initialize the count vector tokenizer with NLTK word tokenize\n        count_vec = CountVectorizer(tokenizer = word_tokenize, token_pattern = None)\n        \n        #fit the count vector\n        count_vec.fit(train_data.TRANS_CONV_TEXT)\n        pickle.dump(count_vec, open(\"count_vectorizer.pickle\", \"wb\")) \n        \n        #transform the training and validation data reviews\n        xtrain = count_vec.transform(train_data.TRANS_CONV_TEXT)\n        xtest = count_vec.transform(test_data.TRANS_CONV_TEXT)\n\n\n        model = linear_model.LogisticRegression()\n        model.fit(xtrain,train_data.Patient_Tag)\n        \n        preds = model.predict(xtest)\n        roc_auc_score = metrics.roc_auc_score(test_data.Patient_Tag,preds)\n        \n        print(f'fold : {fold_}')\n        print(f'roc_auc = {roc_auc_score}')\n        print(classification_report(test_data.Patient_Tag,preds))\n        print(f\"Accuracy Score = {accuracy_score(test_data.Patient_Tag,preds)}\")\n              \n        print('<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>')","execution_count":67,"outputs":[{"output_type":"stream","text":"fold : 0\nroc_auc = 0.882246376811594\n              precision    recall  f1-score   support\n\n           0       0.95      0.97      0.96       184\n           1       0.88      0.79      0.84        48\n\n    accuracy                           0.94       232\n   macro avg       0.92      0.88      0.90       232\nweighted avg       0.93      0.94      0.93       232\n\nAccuracy Score = 0.9353448275862069\n<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>\nfold : 1\nroc_auc = 0.8065232240437159\n              precision    recall  f1-score   support\n\n           0       0.91      0.97      0.94       183\n           1       0.84      0.65      0.73        48\n\n    accuracy                           0.90       231\n   macro avg       0.88      0.81      0.83       231\nweighted avg       0.90      0.90      0.90       231\n\nAccuracy Score = 0.9004329004329005\n<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>\nfold : 2\nroc_auc = 0.8476775956284153\n              precision    recall  f1-score   support\n\n           0       0.94      0.95      0.94       183\n           1       0.78      0.75      0.77        48\n\n    accuracy                           0.90       231\n   macro avg       0.86      0.85      0.85       231\nweighted avg       0.90      0.90      0.90       231\n\nAccuracy Score = 0.9047619047619048\n<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>\nfold : 3\nroc_auc = 0.845457650273224\n              precision    recall  f1-score   support\n\n           0       0.93      0.96      0.95       183\n           1       0.83      0.73      0.78        48\n\n    accuracy                           0.91       231\n   macro avg       0.88      0.85      0.86       231\nweighted avg       0.91      0.91      0.91       231\n\nAccuracy Score = 0.9134199134199135\n<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>\nfold : 4\nroc_auc = 0.8169398907103824\n              precision    recall  f1-score   support\n\n           0       0.92      0.97      0.94       183\n           1       0.84      0.67      0.74        48\n\n    accuracy                           0.90       231\n   macro avg       0.88      0.82      0.84       231\nweighted avg       0.90      0.90      0.90       231\n\nAccuracy Score = 0.9047619047619048\n<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"it seems that, model precision and recall values are not good when it comes to the positive class (Patient label 1). We will make more models to improve the results. "},{"metadata":{},"cell_type":"markdown","source":"Let's submit the first predictions"},{"metadata":{"trusted":true},"cell_type":"code","source":"count_vec = pickle.load(open(\"./count_vectorizer.pickle\", 'rb'))\ntest_features = count_vec.transform(df_test.TRANS_CONV_TEXT)","execution_count":68,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_preds = model.predict(test_features)","execution_count":69,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission_logistic_bow = pd.read_csv('../input/zsdataset/test.csv',encoding = 'ISO-8859-1')","execution_count":70,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"submission = pd.DataFrame()\nsubmission['Index'] = submission_logistic_bow['Index']\nsubmission['Patient_Tag'] = test_preds\nsubmission.to_csv('submission_logistic_bow.csv',index = False)","execution_count":72,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"  We get average ROC AUC score of nearly 86 - 87 percent. We check if it increase by other algorithms"},{"metadata":{},"cell_type":"markdown","source":"### But we can also use Naive bayes classifier, with multinomialNB from Sklearn"},{"metadata":{"trusted":true},"cell_type":"code","source":"df1 = train_df.copy()\ndf1_test = test_df.copy()","execution_count":89,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nfrom sklearn import naive_bayes\nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\n\n\nif __name__ ==\"__main__\":\n    #create a new column with and fill it with -1\n    df1['kfold'] = -1\n    #randomize the data\n    df1 = df1.sample(frac = 1).reset_index(drop =True)\n    y = df1.Patient_Tag.values\n    #initiate model selection class from stratified kfold module\n    kf = model_selection.StratifiedKFold(n_splits = 5)\n    \n    # fill the new k fold column\n    for f,(t_,v_) in enumerate(kf.split(X =df1,y = y)):\n        df1.loc[v_,'kfold'] = f\n\n    #go over the folds created\n    for fold_ in range(5):\n        \n        #creating temporary dataframes for train and test\n        train_data = df1[df1.kfold != fold_].reset_index(drop = True)\n        test_data = df1[df1.kfold == fold_].reset_index(drop = True)\n\n        #initialize the count vector tokenizer with NLTK word tokenize\n        count_vec = CountVectorizer(tokenizer = word_tokenize, token_pattern = None)\n        \n        #fit the count vector\n        count_vec.fit(train_data.TRANS_CONV_TEXT)\n        pickle.dump(count_vec, open(\"count_vectorizer.pickle\", \"wb\"))\n        \n        #transform the training and validation data reviews\n        xtrain = count_vec.transform(train_data.TRANS_CONV_TEXT)\n        xtest = count_vec.transform(test_data.TRANS_CONV_TEXT)\n\n\n        model1 = naive_bayes.MultinomialNB()\n        model1.fit(xtrain,train_data.Patient_Tag)\n        \n        preds = model1.predict(xtest)\n        roc_auc_score = metrics.roc_auc_score(test_data.Patient_Tag,preds)\n        \n        print(f'fold : {fold_}')\n        print(f'roc_auc = {roc_auc_score}')\n        print(classification_report(test_data.Patient_Tag,preds))\n        print(f\"Accuracy Score = {accuracy_score(test_data.Patient_Tag,preds)}\")\n              \n        print('<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>')","execution_count":91,"outputs":[{"output_type":"stream","text":"fold : 0\nroc_auc = 0.9221014492753624\n              precision    recall  f1-score   support\n\n           0       0.99      0.89      0.93       184\n           1       0.69      0.96      0.80        48\n\n    accuracy                           0.90       232\n   macro avg       0.84      0.92      0.87       232\nweighted avg       0.93      0.90      0.91       232\n\nAccuracy Score = 0.9008620689655172\n<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>\nfold : 1\nroc_auc = 0.9014685792349728\n              precision    recall  f1-score   support\n\n           0       0.97      0.91      0.94       183\n           1       0.72      0.90      0.80        48\n\n    accuracy                           0.90       231\n   macro avg       0.84      0.90      0.87       231\nweighted avg       0.92      0.90      0.91       231\n\nAccuracy Score = 0.9047619047619048\n<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>\nfold : 2\nroc_auc = 0.9064207650273223\n              precision    recall  f1-score   support\n\n           0       0.98      0.90      0.93       183\n           1       0.70      0.92      0.79        48\n\n    accuracy                           0.90       231\n   macro avg       0.84      0.91      0.86       231\nweighted avg       0.92      0.90      0.91       231\n\nAccuracy Score = 0.9004329004329005\n<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>\nfold : 3\nroc_auc = 0.9009562841530053\n              precision    recall  f1-score   support\n\n           0       0.98      0.89      0.93       183\n           1       0.68      0.92      0.78        48\n\n    accuracy                           0.89       231\n   macro avg       0.83      0.90      0.85       231\nweighted avg       0.91      0.89      0.90       231\n\nAccuracy Score = 0.8917748917748918\n<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>\nfold : 4\nroc_auc = 0.9036885245901639\n              precision    recall  f1-score   support\n\n           0       0.98      0.89      0.93       183\n           1       0.69      0.92      0.79        48\n\n    accuracy                           0.90       231\n   macro avg       0.83      0.90      0.86       231\nweighted avg       0.92      0.90      0.90       231\n\nAccuracy Score = 0.8961038961038961\n<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"As we can see there is an improvement in the precision and recall values, that is good for us. Let's create submission file"},{"metadata":{"trusted":true},"cell_type":"code","source":"count_vec = pickle.load(open(\"./count_vectorizer.pickle\", 'rb'))\ntest_features = count_vec.transform(df1_test.TRANS_CONV_TEXT)\ntest_preds = model1.predict(test_features)\nsubmission_multinomial_bow = pd.read_csv('../input/zsdataset/test.csv',encoding = 'ISO-8859-1')\nsubmission = pd.DataFrame()\nsubmission['Index'] = submission_logistic_bow['Index']\nsubmission['Patient_Tag'] = test_preds\nsubmission.to_csv('submission_multinomial_bow.csv',index = False)","execution_count":93,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"By using naive bayes and CountVectorizer it reaches to nearly 92 to 93 percentage"},{"metadata":{},"cell_type":"markdown","source":"So for baseline we will use, Naive Bayes instead of logistic regression. Now we will use TF-IDF and Multinomial and check is there an improvement ?"},{"metadata":{},"cell_type":"markdown","source":"### TF - IDF"},{"metadata":{},"cell_type":"markdown","source":"Okay. Let's use, TF-IDF. TF = term frequency. IDF - inverse document frequency. TF = (number of times a term 't' repeats in document / total number of terms in document)\nIDF = (total number of documents / number of document with a term 't' in it)\nTFIDF(t) = TF(t) * IDF(t)"},{"metadata":{},"cell_type":"markdown","source":"Now replace Count vector tokenizer BY tf-idf in naive-bayes"},{"metadata":{"trusted":true},"cell_type":"code","source":"df2 = train_df.copy()\ndf2_test = test_df.copy()","execution_count":106,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nfrom sklearn import linear_model\nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n\n\nif __name__ ==\"__main__\":\n    #create a new column with and fill it with -1\n    df2['kfold'] = -1\n    #randomize the data\n    df2 = df2.sample(frac = 1).reset_index(drop =True)\n    y = df2.Patient_Tag.values\n    #initiate model selection class from stratified kfold module\n    kf = model_selection.StratifiedKFold(n_splits = 5)\n    \n    # fill the new k fold column\n    for f,(t_,v_) in enumerate(kf.split(X =df2,y = y)):\n        df2.loc[v_,'kfold'] = f\n\n    #go over the folds created\n    for fold_ in range(5):\n        \n        #creating temporary dataframes for train and test\n        train_data = df2[df2.kfold != fold_].reset_index(drop = True)\n        test_data = df2[df2.kfold == fold_].reset_index(drop = True)\n\n        #initialize the count vector tokenizer with NLTK word tokenize\n        tfidf_vec = TfidfVectorizer(tokenizer = word_tokenize, token_pattern = None)\n        \n        #fit the count vector\n        tfidf_vec.fit(train_data.TRANS_CONV_TEXT)\n        pickle.dump(tfidf_vec, open(\"tfidf_vectorizer.pickle\", \"wb\"))\n        \n        #transform the training and validation data reviews\n        xtrain = tfidf_vec.transform(train_data.TRANS_CONV_TEXT)\n        xtest = tfidf_vec.transform(test_data.TRANS_CONV_TEXT)\n\n\n        model2 = linear_model.LogisticRegression()\n        model2.fit(xtrain,train_data.Patient_Tag)\n        \n        preds = model2.predict(xtest)\n        roc_auc_score = metrics.roc_auc_score(test_data.Patient_Tag,preds)\n        \n        print(f'fold : {fold_}')\n        print(f'roc_auc = {roc_auc_score}')\n        print(f\"Accuracy Score = {accuracy_score(test_data.Patient_Tag,preds)}\")\n        print(classification_report(test_data.Patient_Tag,preds))\n        \n              \n        print('<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>')","execution_count":108,"outputs":[{"output_type":"stream","text":"fold : 0\nroc_auc = 0.8147644927536233\nAccuracy Score = 0.9137931034482759\n              precision    recall  f1-score   support\n\n           0       0.91      0.98      0.95       184\n           1       0.91      0.65      0.76        48\n\n    accuracy                           0.91       232\n   macro avg       0.91      0.81      0.85       232\nweighted avg       0.91      0.91      0.91       232\n\n<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>\nfold : 1\nroc_auc = 0.8246243169398907\nAccuracy Score = 0.9047619047619048\n              precision    recall  f1-score   support\n\n           0       0.92      0.96      0.94       183\n           1       0.82      0.69      0.75        48\n\n    accuracy                           0.90       231\n   macro avg       0.87      0.82      0.85       231\nweighted avg       0.90      0.90      0.90       231\n\n<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>\nfold : 2\nroc_auc = 0.8273565573770492\nAccuracy Score = 0.9090909090909091\n              precision    recall  f1-score   support\n\n           0       0.92      0.97      0.94       183\n           1       0.85      0.69      0.76        48\n\n    accuracy                           0.91       231\n   macro avg       0.88      0.83      0.85       231\nweighted avg       0.91      0.91      0.91       231\n\n<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>\nfold : 3\nroc_auc = 0.8218920765027322\nAccuracy Score = 0.9004329004329005\n              precision    recall  f1-score   support\n\n           0       0.92      0.96      0.94       183\n           1       0.80      0.69      0.74        48\n\n    accuracy                           0.90       231\n   macro avg       0.86      0.82      0.84       231\nweighted avg       0.90      0.90      0.90       231\n\n<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>\nfold : 4\nroc_auc = 0.7336065573770492\nAccuracy Score = 0.8701298701298701\n              precision    recall  f1-score   support\n\n           0       0.88      0.97      0.92       183\n           1       0.80      0.50      0.62        48\n\n    accuracy                           0.87       231\n   macro avg       0.84      0.73      0.77       231\nweighted avg       0.86      0.87      0.86       231\n\n<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"But it did not give better results, than bag of words :-/ "},{"metadata":{},"cell_type":"markdown","source":"The precision value in patient class is very low. We will save the results, but they are not good. "},{"metadata":{"trusted":true},"cell_type":"code","source":"tfidf_vec = pickle.load(open(\"./tfidf_vectorizer.pickle\", 'rb'))\ntest_features = tfidf_vec.transform(df2_test.TRANS_CONV_TEXT)\ntest_preds = model2.predict(test_features)\nsubmission_logistic_tfidf = pd.read_csv('../input/zsdataset/test.csv',encoding = 'ISO-8859-1')\nsubmission = pd.DataFrame()\nsubmission['Index'] = submission_logistic_tfidf['Index']\nsubmission['Patient_Tag'] = test_preds\nsubmission.to_csv('submission_logistic_tfidf.csv',index = False)","execution_count":109,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's try to add language models to tfidf. We can create bigram models, trigram models and incorporate them with tfidf. We will save it's result if it is good else we will leave them"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nfrom sklearn import linear_model\nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n\n\nif __name__ ==\"__main__\":\n    #create a new column with and fill it with -1\n    df2['kfold'] = -1\n    #randomize the data\n    df2 = df2.sample(frac = 1).reset_index(drop =True)\n    y = df2.Patient_Tag.values\n    #initiate model selection class from stratified kfold module\n    kf = model_selection.StratifiedKFold(n_splits = 5)\n    \n    # fill the new k fold column\n    for f,(t_,v_) in enumerate(kf.split(X =df2,y = y)):\n        df2.loc[v_,'kfold'] = f\n\n    #go over the folds created\n    for fold_ in range(5):\n        \n        #creating temporary dataframes for train and test\n        train_data = df2[df2.kfold != fold_].reset_index(drop = True)\n        test_data = df2[df2.kfold == fold_].reset_index(drop = True)\n\n        #initialize the count vector tokenizer with NLTK word tokenize\n        tfidf_vec = TfidfVectorizer(tokenizer = word_tokenize, token_pattern = None, ngram_range=(1,3))\n        \n        #fit the count vector\n        tfidf_vec.fit(train_data.TRANS_CONV_TEXT)\n        \n        #transform the training and validation data reviews\n        xtrain = count_vec.transform(train_data.TRANS_CONV_TEXT)\n        xtest = count_vec.transform(test_data.TRANS_CONV_TEXT)\n\n\n        model = linear_model.LogisticRegression()\n        model.fit(xtrain,train_data.Patient_Tag)\n        \n        preds = model.predict(xtest)\n        roc_auc_score = metrics.roc_auc_score(test_data.Patient_Tag,preds)\n        \n        print(f'fold : {fold_}')\n        print(f'roc_auc = {roc_auc_score}')\n        print('')","execution_count":24,"outputs":[{"output_type":"stream","text":"fold : 0\nroc_auc = 0.8038949275362319\n\nfold : 1\nroc_auc = 0.7884221311475409\n\nfold : 2\nroc_auc = 0.8739754098360655\n\nfold : 3\nroc_auc = 0.9002732240437159\n\nfold : 4\nroc_auc = 0.8789275956284154\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"it improved the results but didn't beat the naive bayes"},{"metadata":{},"cell_type":"markdown","source":"### Let's try lemmatize and check the results"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem import WordNetLemmatizer\nlemmatizer = WordNetLemmatizer()\ndef lemmatize(sentence):\n    word_list = nltk.word_tokenize(sentence)\n    lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n    return lemmatized_output","execution_count":112,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df3 = train_df.copy()\ndf3['TRANS_CONV_TEXT']=df3['TRANS_CONV_TEXT'].apply(lambda x:lemmatize(x))","execution_count":113,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's check adding lemmatization improves any thing or not."},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nfrom sklearn import naive_bayes \nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\n\n\nif __name__ ==\"__main__\":\n    #create a new column with and fill it with -1\n    df3['kfold'] = -1\n    #randomize the data\n    df3 = df3.sample(frac = 1).reset_index(drop =True)\n    y = df3.Patient_Tag.values\n    #initiate model selection class from stratified kfold module\n    kf = model_selection.StratifiedKFold(n_splits = 5)\n    \n    # fill the new k fold column\n    for f,(t_,v_) in enumerate(kf.split(X =df3,y = y)):\n        df3.loc[v_,'kfold'] = f\n\n    #go over the folds created\n    for fold_ in range(5):\n        \n        #creating temporary dataframes for train and test\n        train_data = df3[df3.kfold != fold_].reset_index(drop = True)\n        test_data = df3[df3.kfold == fold_].reset_index(drop = True)\n\n        #initialize the count vector tokenizer with NLTK word tokenize\n        tfidf_vec = TfidfVectorizer(tokenizer = word_tokenize, token_pattern = None, ngram_range=(1,3))\n        \n        #fit the count vector\n        tfidf_vec.fit(train_data.TRANS_CONV_TEXT)\n        \n        #transform the training and validation data reviews\n        xtrain = count_vec.transform(train_data.TRANS_CONV_TEXT)\n        xtest = count_vec.transform(test_data.TRANS_CONV_TEXT)\n\n\n        model3 = naive_bayes.MultinomialNB()\n        model3.fit(xtrain,train_data.Patient_Tag)\n        \n        preds = model3.predict(xtest)\n        roc_auc_score = metrics.roc_auc_score(test_data.Patient_Tag,preds)\n        \n        print(f'fold : {fold_}')\n        print(f'roc_auc = {roc_auc_score}')\n        print(f\"Accuracy Score = {accuracy_score(test_data.Patient_Tag,preds)}\")\n        print(classification_report(test_data.Patient_Tag,preds))\n        \n              \n        print('<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>')","execution_count":114,"outputs":[{"output_type":"stream","text":"fold : 0\nroc_auc = 0.9012681159420289\nAccuracy Score = 0.8922413793103449\n              precision    recall  f1-score   support\n\n           0       0.98      0.89      0.93       184\n           1       0.68      0.92      0.78        48\n\n    accuracy                           0.89       232\n   macro avg       0.83      0.90      0.85       232\nweighted avg       0.91      0.89      0.90       232\n\n<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>\nfold : 1\nroc_auc = 0.910860655737705\nAccuracy Score = 0.8831168831168831\n              precision    recall  f1-score   support\n\n           0       0.99      0.86      0.92       183\n           1       0.65      0.96      0.77        48\n\n    accuracy                           0.88       231\n   macro avg       0.82      0.91      0.85       231\nweighted avg       0.92      0.88      0.89       231\n\n<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>\nfold : 2\nroc_auc = 0.8582650273224044\nAccuracy Score = 0.8484848484848485\n              precision    recall  f1-score   support\n\n           0       0.96      0.84      0.90       183\n           1       0.59      0.88      0.71        48\n\n    accuracy                           0.85       231\n   macro avg       0.78      0.86      0.80       231\nweighted avg       0.89      0.85      0.86       231\n\n<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>\nfold : 3\nroc_auc = 0.9190573770491803\nAccuracy Score = 0.8961038961038961\n              precision    recall  f1-score   support\n\n           0       0.99      0.88      0.93       183\n           1       0.68      0.96      0.79        48\n\n    accuracy                           0.90       231\n   macro avg       0.83      0.92      0.86       231\nweighted avg       0.92      0.90      0.90       231\n\n<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>\nfold : 4\nroc_auc = 0.850068306010929\nAccuracy Score = 0.8354978354978355\n              precision    recall  f1-score   support\n\n           0       0.96      0.83      0.89       183\n           1       0.57      0.88      0.69        48\n\n    accuracy                           0.84       231\n   macro avg       0.76      0.85      0.79       231\nweighted avg       0.88      0.84      0.85       231\n\n<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Nope not much :-\\ "},{"metadata":{},"cell_type":"markdown","source":"We can also use, snowball stemmering. Lemmatization and stemming both are different.  if word - fishing then stemmed - fish lemma is fishing. We can say stemming is very strict as compared to lemmatization."},{"metadata":{},"cell_type":"markdown","source":"### Let's impliment stemming, and check if it improves scores or not"},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.stem.snowball import SnowballStemmer\nstemmer = SnowballStemmer('english')\ndef stemming(sentence):\n    word_list = nltk.word_tokenize(sentence)\n    stemmed_output = ' '.join([stemmer.stem(w) for w in word_list])\n    return stemmed_output","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df4 = train_df.copy()\ndf4['TRANS_CONV_TEXT']=df4['TRANS_CONV_TEXT'].apply(lambda x: stemming(x))\ndf4_test = test_df.copy()","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from nltk.tokenize import word_tokenize\nfrom sklearn import linear_model\nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import accuracy_score\n\n\nif __name__ ==\"__main__\":\n    #create a new column with and fill it with -1\n    df4['kfold'] = -1\n    #randomize the data\n    df4 = df4.sample(frac = 1).reset_index(drop =True)\n    y = df4.Patient_Tag.values\n    #initiate model selection class from stratified kfold module\n    kf = model_selection.StratifiedKFold(n_splits = 5)\n    \n    # fill the new k fold column\n    for f,(t_,v_) in enumerate(kf.split(X =df4,y = y)):\n        df4.loc[v_,'kfold'] = f\n\n    #go over the folds created\n    for fold_ in range(5):\n        \n        #creating temporary dataframes for train and test\n        train_data = df4[df4.kfold != fold_].reset_index(drop = True)\n        test_data = df4[df4.kfold == fold_].reset_index(drop = True)\n\n        #initialize the count vector tokenizer with NLTK word tokenize\n        count_vec = CountVectorizer(tokenizer = word_tokenize, token_pattern = None, ngram_range=(1,3))\n        \n        #fit the count vector\n        count_vec.fit(train_data.TRANS_CONV_TEXT)\n        \n        pickle.dump(count_vec, open(\"count_vectorizer.pickle\", \"wb\"))\n        #transform the training and validation data reviews\n        xtrain = count_vec.transform(train_data.TRANS_CONV_TEXT)\n        xtest = count_vec.transform(test_data.TRANS_CONV_TEXT)\n\n        \n        model5 = linear_model.LogisticRegression()\n        model5.fit(xtrain,train_data.Patient_Tag)\n        \n        preds = model5.predict(xtest)\n        roc_auc_score = metrics.roc_auc_score(test_data.Patient_Tag,preds)\n        \n        print(f'fold : {fold_}')\n        print(f'roc_auc = {roc_auc_score}')\n        print(f\"Accuracy Score = {accuracy_score(test_data.Patient_Tag,preds)}\")\n        print(classification_report(test_data.Patient_Tag,preds))\n        \n              \n        print('<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>')","execution_count":22,"outputs":[{"output_type":"stream","text":"fold : 0\nroc_auc = 0.8355978260869565\nAccuracy Score = 0.9224137931034483\n              precision    recall  f1-score   support\n\n           0       0.92      0.98      0.95       184\n           1       0.92      0.69      0.79        48\n\n    accuracy                           0.92       232\n   macro avg       0.92      0.84      0.87       232\nweighted avg       0.92      0.92      0.92       232\n\n<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>\nfold : 1\nroc_auc = 0.8191598360655736\nAccuracy Score = 0.8961038961038961\n              precision    recall  f1-score   support\n\n           0       0.92      0.95      0.94       183\n           1       0.79      0.69      0.73        48\n\n    accuracy                           0.90       231\n   macro avg       0.85      0.82      0.83       231\nweighted avg       0.89      0.90      0.89       231\n\n<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>\nfold : 2\nroc_auc = 0.8218920765027322\nAccuracy Score = 0.9004329004329005\n              precision    recall  f1-score   support\n\n           0       0.92      0.96      0.94       183\n           1       0.80      0.69      0.74        48\n\n    accuracy                           0.90       231\n   macro avg       0.86      0.82      0.84       231\nweighted avg       0.90      0.90      0.90       231\n\n<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>\nfold : 3\nroc_auc = 0.876707650273224\nAccuracy Score = 0.9264069264069265\n              precision    recall  f1-score   support\n\n           0       0.95      0.96      0.95       183\n           1       0.84      0.79      0.82        48\n\n    accuracy                           0.93       231\n   macro avg       0.90      0.88      0.89       231\nweighted avg       0.93      0.93      0.93       231\n\n<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>\nfold : 4\nroc_auc = 0.8509221311475409\nAccuracy Score = 0.922077922077922\n              precision    recall  f1-score   support\n\n           0       0.93      0.97      0.95       183\n           1       0.88      0.73      0.80        48\n\n    accuracy                           0.92       231\n   macro avg       0.90      0.85      0.87       231\nweighted avg       0.92      0.92      0.92       231\n\n<<<<<<<<<<<<<<<<<<<<<<>>>>>>>>>>>>>>>>>>>\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Yes, it improved our results. We will implement it in our base models"},{"metadata":{"trusted":true},"cell_type":"code","source":"df4_test['TRANS_CONV_TEXT']=df4_test['TRANS_CONV_TEXT'].apply(lambda x: stemming(x))\ncount_vec = pickle.load(open(\"./count_vectorizer.pickle\", 'rb'))\ntest_features = count_vec.transform(df4_test.TRANS_CONV_TEXT)\ntest_preds = model5.predict(test_features)\nsubmission_stem_count = pd.read_csv('../input/zsdataset/test.csv',encoding = 'ISO-8859-1')\nsubmission = pd.DataFrame()\nsubmission['Index'] = submission_stem_count['Index']\nsubmission['Patient_Tag'] = test_preds\nsubmission.to_csv('submission_stem_count.csv',index = False)","execution_count":23,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"But we can also provide word embeddings. In above all methods, we convered word tokens in numbers. So if there are N unique tokens then they can be represented by integers ranges from 0 to N - 1. We can convert those numbers in to vectors. "},{"metadata":{"trusted":true},"cell_type":"code","source":"df4 = train_df.copy()\ndf4['TRANS_CONV_TEXT']=df4['TRANS_CONV_TEXT'].apply(lambda x: stemming(x))","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\nimport io\nfrom nltk.tokenize import word_tokenize\nfrom sklearn import linear_model\nfrom sklearn import metrics\nfrom sklearn import model_selection\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\ndef sentence_to_vec(s,embedding_dict, stop_words, tokenizer):\n    words = str(s).lower()\n    words = re.sub(r\"[^a-z']+\", ' ', words)\n    words = tokenizer(words)\n    #remove the stop words\n    words = [w for w in words if not w in stop_words]\n    M = []\n    for w in words:\n        #for each word fetch embedding from the dictionary and append the list of embeddings\n        if w in embedding_dict:\n            M.append(embedding_dict[w])\n    #if we dont have vectors then retuen zeros\n    if len(M) == 0:\n        return np.zeros(300)\n    #convert list of embeddings to array\n    M = np.array(M)\n    #calculate the sum over axis = 0\n    v = M.sum(axis = 0)\n    #return normalized vector\n    return v/np.sqrt((v**2).sum())\n\n#for embeddings we need to load vectors\ndef load_vectors(fname):\n    fin = io.open(fname,'r',encoding='utf-8',newline = '\\n', errors = 'ignore')\n    n,d = map(int,fin.readline().split())\n    data = {}\n    for line in fin:\n        tokens = line.strip().split(' ')\n        data[tokens[0]] = list(map(float,tokens[1:]))\n    return data\n\nif __name__ ==\"__main__\":\n    #create a new column with and fill it with -1\n    df4['kfold'] = -1\n    #randomize the data\n    df4 = df4.sample(frac = 1).reset_index(drop =True)\n    # load embeddings into memory\n    print('loading embeddings')\n    embeddings = load_vectors('../input/fasttext-wikinews/wiki-news-300d-1M.vec')\n    vectors = []\n    for TRANS_CONV_TEXT in df4.TRANS_CONV_TEXT.values:\n        vectors.append(sentence_to_vec(s = TRANS_CONV_TEXT, embedding_dict = embeddings, stop_words = [], tokenizer = word_tokenize))\n    vectors = np.array(vectors)\n    \n    y = df4.Patient_Tag.values\n    #initiate model selection class from stratified kfold module\n    kf = model_selection.StratifiedKFold(n_splits = 5)\n    \n    # fill the new k fold column\n    for f,(t_,v_) in enumerate(kf.split(X = vectors,y = y)):\n        print(f'Training fold : {f}')\n        xtrain = vectors[t_,:]\n        ytrain = y[t_]\n        \n        xtest = vectors[v_,:]\n        ytest = y[v_]\n        \n        model = linear_model.LogisticRegression()\n        model.fit(xtrain, ytrain)\n        \n        preds = model.predict(xtest)\n        \n        roc_auc_score = metrics.roc_auc_score(ytest,preds)\n        \n        print(f'fold : {fold_}')\n        print(f'roc_auc = {roc_auc_score}')\n        print('')\n","execution_count":32,"outputs":[{"output_type":"stream","text":"loading embeddings\nTraining fold : 0\nfold : 4\nroc_auc = 0.5828804347826086\n\nTraining fold : 1\nfold : 4\nroc_auc = 0.6612021857923497\n\nTraining fold : 2\nfold : 4\nroc_auc = 0.7028688524590164\n\nTraining fold : 3\nfold : 4\nroc_auc = 0.637636612021858\n\nTraining fold : 4\nfold : 4\nroc_auc = 0.7023565573770492\n\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"I used, fasttext wikinews word vectors, although, other word vectors available, can be used as per convience. But due to memory constraints I tried with this one. Which did not give us good results"},{"metadata":{},"cell_type":"markdown","source":"Now deep learning approach."},{"metadata":{},"cell_type":"markdown","source":"We will make a simple LSTM model and build a model. We will directly use, stemming as it gave us good results previously. And won't take 5 folds now. We will simply divide dataset into train and test where 80% are traing data and 20% as testing data and split it with train test split of sklearn. "},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}